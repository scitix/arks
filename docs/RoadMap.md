# ARKS Project Roadmap
> "The future belongs to those who believe in the beauty of their dreams." â€” Eleanor Roosevelt

## 1. Unified Model Management
Optimize compute efficiency and accelerate inference performance.

- `Multi-source model downloads` â€“ Support fetching models from diverse repositories.
- `Model preloading` â€“ Reduce latency by preloading frequently used models.
- `Near-compute-node acceleration` â€“ Improve the performance of loading models.

## 2. LLM Application Deployment
One-click deployment for simplicity and speed.
- `Multi-runtime support` â€“ Compatible with Dynamo, vLLM, and SGLang.
- `Distributed inference` â€“
    - Multi-node inference (LeaderWorkerSet)
    - PD-separated inference (Dynamo)
- `Heterogeneous inference service deployment` â€“ Run across different hardware types.
- `Extended parameter support` â€“ Flexible runtime optimization.
- `SLO-based autoscaling` â€“ Dynamically adjust resources based on service-level objectives.

## 3. Gateway Features
Intelligent routing and multi-tenant control.

- `Multi-model load balancing` â€“ Efficiently distribute inference requests.
- `Model service discovery` â€“ Automatically detect and route to available instances.
- `Multi-tenant management` â€“
    - Model-level access control
    - Model-level quota management
    - Model-level request rate limiting
- `Tenant domain & certificate management` â€“ Secure and isolate tenant traffic.
- `External SLB support` â€“ Integrate with external load balancers.

## 4. Observability & Monitoring
- `Prometheus integration` â€“ Collect and visualize key metrics.

## 5. Management Console
Centralized control for administrators.
- `Multi-tenant management` â€“ Assign roles, permissions, and resources.
- `User management` â€“ Token-based authentication & quota enforcement.
- `Visual dashboard` â€“ Monitor system health and performance at a glance.

This roadmap outlines our vision for ARKSâ€”a scalable, efficient, and user-friendly platform for AI model deployment and management. ðŸš€
